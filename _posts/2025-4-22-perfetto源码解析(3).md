---
layout: post
title: "perfetto源码解析-基础框架（3）"
categories: perfetto performance
---

上篇介绍了traced运行流程和大体框架，本篇将分析producer的启动流程，以heapprofd为例

代码在src/profiling/memory

从main函数跟踪，很快就看到启动的主体部分。
```
int StartCentralHeapprofd() {
  // We set this up before launching any threads, so we do not have to use a
  // std::atomic for g_dump_evt.
  g_dump_evt = new base::EventFd();

  base::UnixTaskRunner task_runner;
  base::Watchdog::GetInstance()->Start();  // crash on exceedingly long tasks
  HeapprofdProducer producer(HeapprofdMode::kCentral, &task_runner,
                             /* exit_when_done= */ false);

  int listening_raw_socket = GetListeningSocket();
  auto listening_socket = base::UnixSocket::Listen(
      base::ScopedFile(listening_raw_socket), &producer.socket_delegate(),
      &task_runner, base::SockFamily::kUnix, base::SockType::kStream);

  struct sigaction action = {};
  action.sa_handler = [](int) { g_dump_evt->Notify(); };
  // Allow to trigger a full dump by sending SIGUSR1 to heapprofd.
  // This will allow manually deciding when to dump on userdebug.
  PERFETTO_CHECK(sigaction(SIGUSR1, &action, nullptr) == 0);
  task_runner.AddFileDescriptorWatch(g_dump_evt->fd(), [&producer] {
    g_dump_evt->Clear();
    producer.DumpAll();
  });
  producer.ConnectWithRetries(GetProducerSocket());
  // TODO(fmayer): Create one producer that manages both heapprofd and Java
  // producers, so we do not have two connections to traced.
  JavaHprofProducer java_producer(&task_runner);
  java_producer.ConnectWithRetries(GetProducerSocket());
  task_runner.Run();
  return 0;
}
```
<!--more-->
这一部分首先启了个Watchdog，这部分不详细解释了，

然后创建了HeapprofdProducer对象，接着监听了/dev/socket/heapprofd，base::UnixSocket::Listen上一篇已经提到过，这里不展开，只是要注意，监听socket后续的事件会由producer.socket_delegate()处理，这部分也在后面讨论。

接着注册了信号处理函数处理SIGUSR1，处理方法就是由task_runner去运行producer.DumpAll()

然后执行producer.ConnectWithRetries去连接/dev/socket/traced_producer

下面同样又创建了JavaHprofProducer并去连接/dev/socket/traced_producer

接着就进入了task_runner处理各种异步事件

看HeapprofdProducer::ConnectWithRetries，设置了状态，执行ConnectService
```
void HeapprofdProducer::ConnectWithRetries(const char* socket_name) {
  PERFETTO_DCHECK(state_ == kNotStarted);
  state_ = kNotConnected;

  ResetConnectionBackoff();
  producer_sock_name_ = socket_name;
  ConnectService();
}
```
再看ConnectService，这里能看到DataSource名称"android.heapprofd"
```
void HeapprofdProducer::ConnectService() {
  SetProducerEndpoint(ProducerIPCClient::Connect(
      producer_sock_name_, this, "android.heapprofd", task_runner_));
}
```
ProducerIPCClient::Connect里面直接创建ProducerIPCClientImpl对象，看它的构造函数，这里主要创建了Client的实例，然后调用了BindService。
```
ipc_channel_ =
    ipc::Client::CreateInstance(std::move(conn_args), task_runner);
ipc_channel_->BindService(producer_port_->GetWeakPtr());
```
先看CreateInstance，这里可以看到ClientImpl，还记得前面service使用的HostImpl吗，这里正是和它对应的Client实现
```
std::unique_ptr<Client> Client::CreateInstance(ConnArgs conn_args,
                                               base::TaskRunner* task_runner) {
  std::unique_ptr<Client> client(
      new ClientImpl(std::move(conn_args), task_runner));
  return client;
}
```
接着看ClientImpl的构造函数，这里用的else里的TryConnect，而TryConnect调用了UnixSocket::Connect，注意这里的第二个参数this是EventListener，即ClientImpl处理socket事件的回调。
```
ClientImpl::ClientImpl(ConnArgs conn_args, base::TaskRunner* task_runner)
    : socket_name_(conn_args.socket_name),
      socket_retry_(conn_args.retry),
      task_runner_(task_runner),
      weak_ptr_factory_(this) {
  if (conn_args.socket_fd) {
    // Create the client using a connected socket. This code path will never hit
    // OnConnect().
    sock_ = base::UnixSocket::AdoptConnected(
        std::move(conn_args.socket_fd), this, task_runner_, kClientSockFamily,
        base::SockType::kStream, base::SockPeerCredMode::kIgnore);
  } else {
    // Connect using the socket name.
    TryConnect();
  }
}

void ClientImpl::TryConnect() {
  PERFETTO_DCHECK(socket_name_);
  sock_ = base::UnixSocket::Connect(
      socket_name_, this, task_runner_, base::GetSockFamily(socket_name_),
      base::SockType::kStream, base::SockPeerCredMode::kIgnore);
}
```
UnixSocket::Connect里首先创建了UnixSocket对象，然后调用了它的DoConnect

先看UnixSocket的构造函数，这里adopt_state传进来是kDisconnected，所以会创建一个socket,设为非阻塞，并且把它添加到task_runner_的监听列表中，还记得service socket这里做的是什么吗？前面提到service里adopt_state传进来是kListening。
```
  state_ = State::kDisconnected;
  if (adopt_state == State::kDisconnected) {
    PERFETTO_DCHECK(!adopt_fd);
    sock_raw_ = UnixSocketRaw::CreateMayFail(sock_family, sock_type);
    if (!sock_raw_)
      return;
  } else if (adopt_state == State::kConnected) {
...
  PERFETTO_CHECK(sock_raw_);
  sock_raw_.SetBlocking(false);
  WeakPtr<UnixSocket> weak_ptr = weak_ptr_factory_.GetWeakPtr();
  task_runner_->AddFileDescriptorWatch(sock_raw_.watch_handle(), [weak_ptr] {
    if (weak_ptr)
      weak_ptr->OnEvent();
  });
```
上面的构造函数并没有连接service,这部分在DoConnect里做了，并且把state_从kDisconnected切换到了kConnecting，这里有一大段注释省略了，大概意思是解释了下面为什么要在task_runner_里执行一次OnEvent，OnEvent是socket状态有变化时的直接回调。
```
void UnixSocket::DoConnect(const std::string& socket_name) {
  PERFETTO_DCHECK(state_ == State::kDisconnected);

  // This is the only thing that can gracefully fail in the ctor.
  if (!sock_raw_)
    return NotifyConnectionState(false);

  if (!sock_raw_.Connect(socket_name))
    return NotifyConnectionState(false);

  // At this point either connect() succeeded or started asynchronously
  // (errno = EINPROGRESS).
  state_ = State::kConnecting;
...
  WeakPtr<UnixSocket> weak_ptr = weak_ptr_factory_.GetWeakPtr();
  task_runner_->PostTask([weak_ptr] {
    if (weak_ptr)
      weak_ptr->OnEvent();
  });
}
```
至此socket初始化和连接部分结束了，回到ProducerIPCClient::Connect那里，接着调用了ClientImpl::BindService，还记得前面socket的状态是什么吗？

是kConnecting，所以这里也就是吧service_proxy添加到queued_bindings_等后续做处理
```
void ClientImpl::BindService(base::WeakPtr<ServiceProxy> service_proxy) {
  if (!service_proxy)
    return;
  if (!sock_->is_connected()) {
    queued_bindings_.emplace_back(service_proxy);
    return;
  }
  ...
}
```
未完待续
