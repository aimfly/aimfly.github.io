---
layout: post
title: "perfetto源码解析-基础框架（3）"
categories: perfetto performance
---

上篇介绍了traced运行流程和大体框架，本篇将分析producer的启动流程，以heapprofd为例

代码在src/profiling/memory

从main函数跟踪，很快就看到启动的主体部分。
```
int StartCentralHeapprofd() {
  // We set this up before launching any threads, so we do not have to use a
  // std::atomic for g_dump_evt.
  g_dump_evt = new base::EventFd();

  base::UnixTaskRunner task_runner;
  base::Watchdog::GetInstance()->Start();  // crash on exceedingly long tasks
  HeapprofdProducer producer(HeapprofdMode::kCentral, &task_runner,
                             /* exit_when_done= */ false);

  int listening_raw_socket = GetListeningSocket();
  auto listening_socket = base::UnixSocket::Listen(
      base::ScopedFile(listening_raw_socket), &producer.socket_delegate(),
      &task_runner, base::SockFamily::kUnix, base::SockType::kStream);

  struct sigaction action = {};
  action.sa_handler = [](int) { g_dump_evt->Notify(); };
  // Allow to trigger a full dump by sending SIGUSR1 to heapprofd.
  // This will allow manually deciding when to dump on userdebug.
  PERFETTO_CHECK(sigaction(SIGUSR1, &action, nullptr) == 0);
  task_runner.AddFileDescriptorWatch(g_dump_evt->fd(), [&producer] {
    g_dump_evt->Clear();
    producer.DumpAll();
  });
  producer.ConnectWithRetries(GetProducerSocket());
  // TODO(fmayer): Create one producer that manages both heapprofd and Java
  // producers, so we do not have two connections to traced.
  JavaHprofProducer java_producer(&task_runner);
  java_producer.ConnectWithRetries(GetProducerSocket());
  task_runner.Run();
  return 0;
}
```
<!--more-->
这一部分首先启了个Watchdog，这部分不详细解释了，

然后创建了HeapprofdProducer对象，接着监听了/dev/socket/heapprofd，base::UnixSocket::Listen上一篇已经提到过，这里不展开，只是要注意，监听socket后续的事件会由producer.socket_delegate()处理，这部分也在后面讨论。

接着注册了信号处理函数处理SIGUSR1，处理方法就是由task_runner去运行producer.DumpAll()

然后执行producer.ConnectWithRetries去连接/dev/socket/traced_producer

下面同样又创建了JavaHprofProducer并去连接/dev/socket/traced_producer

接着就进入了task_runner处理各种异步事件

看HeapprofdProducer::ConnectWithRetries，设置了状态，执行ConnectService
```
void HeapprofdProducer::ConnectWithRetries(const char* socket_name) {
  PERFETTO_DCHECK(state_ == kNotStarted);
  state_ = kNotConnected;

  ResetConnectionBackoff();
  producer_sock_name_ = socket_name;
  ConnectService();
}
```
再看ConnectService，这里能看到DataSource名称"android.heapprofd"
```
void HeapprofdProducer::ConnectService() {
  SetProducerEndpoint(ProducerIPCClient::Connect(
      producer_sock_name_, this, "android.heapprofd", task_runner_));
}
```
ProducerIPCClient::Connect里面直接创建ProducerIPCClientImpl对象，看它的构造函数，这里主要创建了Client的实例，然后调用了BindService。
```
ipc_channel_ =
    ipc::Client::CreateInstance(std::move(conn_args), task_runner);
ipc_channel_->BindService(producer_port_->GetWeakPtr());
```
先看CreateInstance，这里可以看到ClientImpl，还记得前面service使用的HostImpl吗，这里正是和它对应的Client实现
```
std::unique_ptr<Client> Client::CreateInstance(ConnArgs conn_args,
                                               base::TaskRunner* task_runner) {
  std::unique_ptr<Client> client(
      new ClientImpl(std::move(conn_args), task_runner));
  return client;
}
```
接着看ClientImpl的构造函数，这里用的else里的TryConnect，而TryConnect调用了UnixSocket::Connect，注意这里的第二个参数this是EventListener，即ClientImpl处理socket事件的回调。
```
ClientImpl::ClientImpl(ConnArgs conn_args, base::TaskRunner* task_runner)
    : socket_name_(conn_args.socket_name),
      socket_retry_(conn_args.retry),
      task_runner_(task_runner),
      weak_ptr_factory_(this) {
  if (conn_args.socket_fd) {
    // Create the client using a connected socket. This code path will never hit
    // OnConnect().
    sock_ = base::UnixSocket::AdoptConnected(
        std::move(conn_args.socket_fd), this, task_runner_, kClientSockFamily,
        base::SockType::kStream, base::SockPeerCredMode::kIgnore);
  } else {
    // Connect using the socket name.
    TryConnect();
  }
}

void ClientImpl::TryConnect() {
  PERFETTO_DCHECK(socket_name_);
  sock_ = base::UnixSocket::Connect(
      socket_name_, this, task_runner_, base::GetSockFamily(socket_name_),
      base::SockType::kStream, base::SockPeerCredMode::kIgnore);
}
```
UnixSocket::Connect里首先创建了UnixSocket对象，然后调用了它的DoConnect

先看UnixSocket的构造函数，这里adopt_state传进来是kDisconnected，所以会创建一个socket,设为非阻塞，并且把它添加到task_runner_的监听列表中，还记得service socket这里做的是什么吗？前面提到service里adopt_state传进来是kListening。
```
  state_ = State::kDisconnected;
  if (adopt_state == State::kDisconnected) {
    PERFETTO_DCHECK(!adopt_fd);
    sock_raw_ = UnixSocketRaw::CreateMayFail(sock_family, sock_type);
    if (!sock_raw_)
      return;
  } else if (adopt_state == State::kConnected) {
...
  PERFETTO_CHECK(sock_raw_);
  sock_raw_.SetBlocking(false);
  WeakPtr<UnixSocket> weak_ptr = weak_ptr_factory_.GetWeakPtr();
  task_runner_->AddFileDescriptorWatch(sock_raw_.watch_handle(), [weak_ptr] {
    if (weak_ptr)
      weak_ptr->OnEvent();
  });
```
上面的构造函数并没有连接service,这部分在DoConnect里做了，并且把state_从kDisconnected切换到了kConnecting，这里有一大段注释省略了，大概意思是解释了下面为什么要在task_runner_里执行一次OnEvent，OnEvent是socket状态有变化时的直接回调。
```
void UnixSocket::DoConnect(const std::string& socket_name) {
  PERFETTO_DCHECK(state_ == State::kDisconnected);

  // This is the only thing that can gracefully fail in the ctor.
  if (!sock_raw_)
    return NotifyConnectionState(false);

  if (!sock_raw_.Connect(socket_name))
    return NotifyConnectionState(false);

  // At this point either connect() succeeded or started asynchronously
  // (errno = EINPROGRESS).
  state_ = State::kConnecting;
...
  WeakPtr<UnixSocket> weak_ptr = weak_ptr_factory_.GetWeakPtr();
  task_runner_->PostTask([weak_ptr] {
    if (weak_ptr)
      weak_ptr->OnEvent();
  });
}
```
至此socket初始化和连接部分结束了，回到ProducerIPCClient::Connect那里，接着调用了ClientImpl::BindService，还记得前面socket的状态是什么吗？

是kConnecting，所以这里也就是把service_proxy添加到queued_bindings_等后续做处理
```
void ClientImpl::BindService(base::WeakPtr<ServiceProxy> service_proxy) {
  if (!service_proxy)
    return;
  if (!sock_->is_connected()) {
    queued_bindings_.emplace_back(service_proxy);
    return;
  }
  ...
}
```
这个service_proxy是什么呢，它的类型是protos::gen::ProducerPortProxy，也是protobuf自动生产 

它是ProducerIPCClientImpl初始化时创建的用于和service收发消息的代理对象，并且看到这个对象接受了ProducerIPCClientImpl对象做为回调事件的处理
```
      producer_port_(
          new protos::gen::ProducerPortProxy(this /* event_listener */)),
```
这之后就会进入task_runner来处理各种异步事件，这里总结一下各个类的关系方便下面继续分析。

最外层的是HeapprofdProducer对象，它包含了对heap profiler的一些业务流的处理，这部分会在其他章节说明。

接下来是ProducerIPCClientImpl对象处理producer客户端相关的业务。它又是protos::gen::ProducerPortProxy的事件监听者。

然后是ClientImpl处理客户端业务。它会使用ProducerPortProxy来和服务端通信。

最后是UnixSocket处理socket事件。

刚才提到socket连接后进入了kConnecting状态，然后task_runner紧接着会回调UnixSocket::OnEvent

来看它的处理，这里正常会执行回调event_listener_->OnConnect(this, true /* connected */)，而event_listener_就是ClientImpl
```
  if (state_ == State::kConnecting) {
    PERFETTO_DCHECK(sock_raw_);
    int sock_err = EINVAL;
    socklen_t err_len = sizeof(sock_err);
    int res =
        getsockopt(sock_raw_.fd(), SOL_SOCKET, SO_ERROR, &sock_err, &err_len);

    if (res == 0 && sock_err == EINPROGRESS)
      return;  // Not connected yet, just a spurious FD watch wakeup.
    if (res == 0 && sock_err == 0) {
      if (peer_cred_mode_ == SockPeerCredMode::kReadOnConnect)
        ReadPeerCredentialsPosix();
      state_ = State::kConnected;
      return event_listener_->OnConnect(this, true /* connected */);
    }
    PERFETTO_DLOG("Connection error: %s", strerror(sock_err));
    Shutdown(false);
    return event_listener_->OnConnect(this, false /* connected */);
  }
```
看ClientImpl::OnConnect，这里又执行了BindService
```
  auto queued_bindings = std::move(queued_bindings_);
  queued_bindings_.clear();
  for (base::WeakPtr<ServiceProxy>& service_proxy : queued_bindings) {
    if (connected) {
      BindService(service_proxy);
    } else if (service_proxy) {
      service_proxy->OnConnect(false /* success */);
    }
  }
```
接着看BindService另一部分，这里看到组装了带有bind_service消息的protos::gen::IPCFrame并且调用了SendFrame发送，然后把request放进了queued_requests_
```
  RequestID request_id = ++last_request_id_;
  Frame frame;
  frame.set_request_id(request_id);
  Frame::BindService* req = frame.mutable_msg_bind_service();
  const char* const service_name = service_proxy->GetDescriptor().service_name;
  req->set_service_name(service_name);
  if (!SendFrame(frame)) {
    PERFETTO_DLOG("BindService(%s) failed", service_name);
    return service_proxy->OnConnect(false /* success */);
  }
  QueuedRequest qr;
  qr.type = Frame::kMsgBindServiceFieldNumber;
  qr.request_id = request_id;
  qr.service_proxy = service_proxy;
  queued_requests_.emplace(request_id, std::move(qr));
```
SendFrame下面就是做了序列化和发送，不细说了，然后看server端处理，也就是traced。

前面章节也提到过服务端收到数据会调用UnixSocket::OnEvent，然后调用它事件监听者即HostImpl的OnDataAvailable，然后调用OnReceivedFrame。

HostImpl::OnReceivedFrame比较简单，它这里会调用OnBindService，这里直接组装msg_bind_service_reply回复，回复中带了service id和method ids。
```
void HostImpl::OnBindService(ClientConnection* client, const Frame& req_frame) {
  // Binding a service doesn't do anything major. It just returns back the
  // service id and its method map.
  const Frame::BindService& req = req_frame.msg_bind_service();
  Frame reply_frame;
  reply_frame.set_request_id(req_frame.request_id());
  auto* reply = reply_frame.mutable_msg_bind_service_reply();
  const ExposedService* service = GetServiceByName(req.service_name());
  if (service) {
    reply->set_success(true);
    reply->set_service_id(service->id);
    uint32_t method_id = 1;  // method ids start at index 1.
    for (const auto& desc_method : service->instance->GetDescriptor().methods) {
      Frame::BindServiceReply::MethodInfo* method_info = reply->add_methods();
      method_info->set_name(desc_method.name);
      method_info->set_id(method_id++);
    }
  }
  SendFrame(client, reply_frame);
}
```
接着看客户端的处理，还记得之前发请求时存过queued_requests_，这里先拿到reply对应的request，接着用request和reply调用了OnBindServiceReply
```
void ClientImpl::OnFrameReceived(const Frame& frame) {
  auto queued_requests_it = queued_requests_.find(frame.request_id());
  if (queued_requests_it == queued_requests_.end()) {
    PERFETTO_DLOG("OnFrameReceived(): got invalid request_id=%" PRIu64,
                  static_cast<uint64_t>(frame.request_id()));
    return;
  }
  QueuedRequest req = std::move(queued_requests_it->second);
  queued_requests_.erase(queued_requests_it);

  if (req.type == Frame::kMsgBindServiceFieldNumber &&
      frame.has_msg_bind_service_reply()) {
    return OnBindServiceReply(std::move(req), frame.msg_bind_service_reply());
  }
  if (req.type == Frame::kMsgInvokeMethodFieldNumber &&
      frame.has_msg_invoke_method_reply()) {
    return OnInvokeMethodReply(std::move(req), frame.msg_invoke_method_reply());
  }
...
}
```
接着看OnBindServiceReply，这里主要就是用服务端发回的service id和method ids初始化service_proxy，即ProducerPortProxy，目的是请求服务端执行某方法时，确保方法存在。同时存入service_bindings_
```
void ClientImpl::OnBindServiceReply(QueuedRequest req,
                                    const Frame::BindServiceReply& reply) {
  base::WeakPtr<ServiceProxy>& service_proxy = req.service_proxy;
  if (!service_proxy)
    return;
  const char* svc_name = service_proxy->GetDescriptor().service_name;
  if (!reply.success()) {
    PERFETTO_DLOG("BindService(): unknown service_name=\"%s\"", svc_name);
    return service_proxy->OnConnect(false /* success */);
  }

  auto prev_service = service_bindings_.find(reply.service_id());
  if (prev_service != service_bindings_.end() && prev_service->second.get()) {
    PERFETTO_DLOG(
        "BindService(): Trying to bind service \"%s\" but another service "
        "named \"%s\" is already bound with the same ID.",
        svc_name, prev_service->second->GetDescriptor().service_name);
    return service_proxy->OnConnect(false /* success */);
  }

  // Build the method [name] -> [remote_id] map.
  std::map<std::string, MethodID> methods;
  for (const auto& method : reply.methods()) {
    if (method.name().empty() || method.id() <= 0) {
      PERFETTO_DLOG("OnBindServiceReply(): invalid method \"%s\" -> %" PRIu64,
                    method.name().c_str(), static_cast<uint64_t>(method.id()));
      continue;
    }
    methods[method.name()] = method.id();
  }
  service_proxy->InitializeBinding(weak_ptr_factory_.GetWeakPtr(),
                                   reply.service_id(), std::move(methods));
  service_bindings_[reply.service_id()] = service_proxy;
  service_proxy->OnConnect(true /* success */);
}
```
接着看service_proxy->OnConnect，ProducerPortProxy继承了ServiceProxy，所以这段代码在ServiceProxy里

还记得这里事件监听者是什么么，没错，是ProducerIPCClientImpl
```
void ServiceProxy::OnConnect(bool success) {
  if (success) {
    PERFETTO_DCHECK(service_id_);
    return event_listener_->OnConnect();
  }
  return event_listener_->OnDisconnect();
}
```
来看ProducerIPCClientImpl::OnConnect，这里主要做了两件事，一是调用了ProducerPortProxy的InitializeConnection，
二是调用了GetAsyncCommand，并且分别注册了回调，GetAsyncCommand非常重要，它告诉服务端已经准备好接受后续命令，并且在回调中处理后续命令，例如SetupDataSource，StartTracing等。
```
void ProducerIPCClientImpl::OnConnect() {
  PERFETTO_DCHECK_THREAD(thread_checker_);
  connected_ = true;

  ipc::Deferred<protos::gen::InitializeConnectionResponse> on_init;
  on_init.Bind(
      [this](ipc::AsyncResult<protos::gen::InitializeConnectionResponse> resp) {
        OnConnectionInitialized(
            resp.success(),
            resp.success() ? resp->using_shmem_provided_by_producer() : false,
            resp.success() ? resp->direct_smb_patching_supported() : false,
            resp.success() ? resp->use_shmem_emulation() : false);
      });
  protos::gen::InitializeConnectionRequest req;
  req.set_producer_name(name_);
  req.set_shared_memory_size_hint_bytes(
      static_cast<uint32_t>(shared_memory_size_hint_bytes_));
  req.set_shared_memory_page_size_hint_bytes(
      static_cast<uint32_t>(shared_memory_page_size_hint_bytes_));
  ...

  req.set_sdk_version(base::GetVersionString());
  producer_port_->InitializeConnection(req, std::move(on_init), shm_fd);

  // Create the back channel to receive commands from the Service.
  ipc::Deferred<protos::gen::GetAsyncCommandResponse> on_cmd;
  on_cmd.Bind(
      [this](ipc::AsyncResult<protos::gen::GetAsyncCommandResponse> resp) {
        if (!resp)
          return;  // The IPC channel was closed and |resp| was auto-rejected.
        OnServiceRequest(*resp);
      });
  producer_port_->GetAsyncCommand(protos::gen::GetAsyncCommandRequest(),
                                  std::move(on_cmd));

  // If there are pending Sync() requests, send them now.
  for (const auto& pending_sync : pending_sync_reqs_)
    Sync(std::move(pending_sync));
  pending_sync_reqs_.clear();
}
```
未完待续
